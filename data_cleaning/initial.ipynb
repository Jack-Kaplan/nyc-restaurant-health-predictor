{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NYC Restaurant Health Inspection Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.width', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# API Configuration\n",
    "# =============================================================================\n",
    "API_ENDPOINT = \"https://data.cityofnewyork.us/api/v3/views/43nn-pn8j/query.csv\"\n",
    "MAX_RETRIES = 3\n",
    "RETRY_DELAY_SECONDS = 5\n",
    "REQUEST_TIMEOUT_SECONDS = 300  # 5 min timeout for large CSV download\n",
    "\n",
    "# Cache configuration\n",
    "CACHE_DIR = \"../data\"\n",
    "CACHE_FILENAME = \"nyc_restaurant_inspections_cached.csv\"\n",
    "CACHE_PATH = os.path.join(CACHE_DIR, CACHE_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_app_token(token=None):\n",
    "    \"\"\"\n",
    "    Get NYC Open Data API token.\n",
    "    \n",
    "    Priority order:\n",
    "    1. Directly passed token parameter\n",
    "    2. NYC_OPENDATA_APP_TOKEN environment variable\n",
    "    3. .env file in parent directory (for local testing)\n",
    "        If using .end file, create a file at ../data/.env with the content:\n",
    "            NYC_OPENDATA_APP_TOKEN=your_token_here\n",
    "    4. Raise error if not found\n",
    "    \"\"\"\n",
    "    # 1. Direct token\n",
    "    if token:\n",
    "        return token\n",
    "    \n",
    "    # 2. Environment variable\n",
    "    env_token = os.environ.get('NYC_OPENDATA_APP_TOKEN')\n",
    "    if env_token:\n",
    "        return env_token\n",
    "    \n",
    "    # 3. Try .env file for local testing\n",
    "    env_file = os.path.join(CACHE_DIR, '.env')\n",
    "    if os.path.exists(env_file):\n",
    "        with open(env_file, 'r') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line.startswith('NYC_OPENDATA_APP_TOKEN='):\n",
    "                    return line.split('=', 1)[1].strip().strip('\"').strip(\"'\")\n",
    "    \n",
    "    raise ValueError(\n",
    "        \"NYC Open Data API token not found.\\n\\n\"\n",
    "        \"Set it using one of these methods:\\n\"\n",
    "        \"  1. Pass directly: load_restaurant_data(token='your_token')\\n\"\n",
    "        \"  2. Environment variable: export NYC_OPENDATA_APP_TOKEN='your_token'\\n\"\n",
    "        \"  3. Create ../data/.env file with: NYC_OPENDATA_APP_TOKEN=your_token\"\n",
    "    )\n",
    "\n",
    "\n",
    "def fetch_all_restaurant_data(app_token):\n",
    "    \"\"\"Fetch all restaurant inspection data from NYC Open Data API as CSV.\"\"\"\n",
    "    from io import StringIO\n",
    "    \n",
    "    print(\"Fetching data from NYC Open Data API...\")\n",
    "    print(f\"Endpoint: {API_ENDPOINT}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    headers = {\n",
    "        'X-App-Token': app_token\n",
    "    }\n",
    "    \n",
    "    # For CSV endpoint, use GET with query params\n",
    "    params = {\n",
    "        '$limit': 500000,  # Get all records\n",
    "    }\n",
    "    \n",
    "    for attempt in range(MAX_RETRIES):\n",
    "        try:\n",
    "            response = requests.get(\n",
    "                API_ENDPOINT,\n",
    "                headers=headers,\n",
    "                params=params,\n",
    "                timeout=REQUEST_TIMEOUT_SECONDS\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            break\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            if attempt < MAX_RETRIES - 1:\n",
    "                wait_time = RETRY_DELAY_SECONDS * (2 ** attempt)\n",
    "                print(f\"  Request failed: {e}. Retrying in {wait_time}s...\")\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                raise\n",
    "    \n",
    "    # Parse CSV response\n",
    "    df = pd.read_csv(StringIO(response.text))\n",
    "    \n",
    "    print(f\"Fetch complete! Total records: {len(df):,}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_restaurant_data(force_refresh=False, token=None):\n",
    "    \"\"\"\n",
    "    Load restaurant inspection data with caching.\n",
    "    \n",
    "    If cached data exists locally, load from cache.\n",
    "    Otherwise, fetch from NYC Open Data API and cache locally.\n",
    "    \n",
    "    Args:\n",
    "        force_refresh: If True, fetch fresh data from API even if cache exists\n",
    "        token: Optional API token for local testing (bypasses env var lookup)\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Restaurant inspection data\n",
    "    \"\"\"\n",
    "    # Ensure data directory exists\n",
    "    if not os.path.exists(CACHE_DIR):\n",
    "        os.makedirs(CACHE_DIR)\n",
    "    \n",
    "    # Check for cached data\n",
    "    if not force_refresh and os.path.exists(CACHE_PATH):\n",
    "        print(f\"Loading data from cache: {CACHE_PATH}\")\n",
    "        df = pd.read_csv(CACHE_PATH, low_memory=False)\n",
    "        print(f\"Loaded {len(df):,} records from cache\")\n",
    "        print(\"Set force_refresh=True to fetch fresh data from API\")\n",
    "        return df\n",
    "    \n",
    "    # Fetch from API\n",
    "    print(\"No cached data found. Fetching from API...\")\n",
    "    app_token = get_app_token(token)\n",
    "    df = fetch_all_restaurant_data(app_token)\n",
    "    \n",
    "    # Save to cache\n",
    "    df.to_csv(CACHE_PATH, index=False)\n",
    "    print(f\"\\nData cached to: {CACHE_PATH}\")\n",
    "    print(f\"Cache size: {os.path.getsize(CACHE_PATH) / (1024*1024):.1f} MB\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from NYC Open Data API (or cache if available)\n",
    "# Set force_refresh=True to re-download latest data from API\n",
    "df = load_restaurant_data(force_refresh=False)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DATA LOADED SUCCESSFULLY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"\\nColumn names: {list(df.columns)}\")\n",
    "print(f\"\\nData types:\\n{df.dtypes}\")\n",
    "print(f\"\\nMissing values:\\n{df.isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Explore Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Initial Filtering\n",
    "\n",
    "Based on the dataset dictionary, we will:\n",
    "1. **Drop unnecessary columns** not relevant to grade prediction\n",
    "2. **Remove placeholder inspection dates** (01/01/1900)\n",
    "3. **Keep only Cycle Inspections** - these are the regular health inspections that result in grades (A/B/C). Other inspection types (Smoke-Free Air Act, Inter-Agency Task Force, etc.) don't produce health grades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "df_copy = df.copy()\n\n# We need to set all columns to lowercase for consistency, and replace spaces with underscores\ndf_copy.columns = df_copy.columns.str.lower().str.replace(' ', '_')\n\n# Drop unnecessary columns (keep latitude, longitude for map display)\ndrop_columns = ['phone', 'action', 'record_date', 'community_board', 'council_district', \n                'census_tract', 'bin', 'bbl', 'nta', 'location']\ndf_copy = df_copy.drop(columns=drop_columns)\n\nprint(f\"Original shape: {df_copy.shape}\")\n\n# Remove placeholder inspection dates\ndrop_rows = df_copy[df_copy['inspection_date'] == '01/01/1900'].index\ndf_copy = df_copy.drop(index=drop_rows)\nprint(f\"After removing placeholder dates: {df_copy.shape} (removed {len(drop_rows):,})\")\n\n# Keep only Cycle Inspections (the only ones that produce health grades)\nbefore_count = len(df_copy)\ndf_copy = df_copy[df_copy['inspection_type'].str.contains('Cycle Inspection', case=False, na=False)]\nprint(f\"After filtering to Cycle Inspections only: {df_copy.shape} (removed {before_count - len(df_copy):,})\")\n\ndf_copy.head()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Converting Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Convert date columns to datetime\n# Let pandas infer the date format automatically\ndf_copy['inspection_date'] = pd.to_datetime(df_copy['inspection_date'])\ndf_copy['grade_date'] = pd.to_datetime(df_copy['grade_date'], errors='coerce')\n\n# Convert ZIPCODE: clean numeric conversion, then to 5-digit string\n# First convert to numeric (handles floats like 10001.0), then to int, then to zero-padded string\ndf_copy['zipcode'] = pd.to_numeric(df_copy['zipcode'], errors='coerce')\ndf_copy['zipcode'] = df_copy['zipcode'].apply(\n    lambda x: str(int(x)).zfill(5) if pd.notna(x) else None\n)\n\n# Convert CAMIS to string (it's an ID, not a number)\ndf_copy['camis'] = df_copy['camis'].astype(str)\n\nprint(\"Data types after conversion:\")\nprint(df_copy.dtypes)\nprint(f\"\\nZipcode sample: {df_copy['zipcode'].dropna().head(10).tolist()}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Check Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values\n",
    "print(\"Missing values by column:\")\n",
    "print(df_copy.isnull().sum())\n",
    "print(f\"\\nTotal rows: {len(df_copy):,}\")\n",
    "\n",
    "# Note: Some missing grades are expected for initial inspections that haven't been graded yet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Data Validation and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Trim whitespace from text columns\ntext_cols = ['dba', 'street', 'building', 'cuisine_description', 'violation_description']\nfor col in text_cols:\n    if col in df_copy.columns:\n        df_copy[col] = df_copy[col].str.strip()\n\n# Filter to valid NYC coordinates (remove nulls and invalid 0,0 values)\nbefore_count = len(df_copy)\ndf_copy = df_copy.dropna(subset=['latitude', 'longitude'])\ndf_copy = df_copy[(df_copy['latitude'] > 40) & (df_copy['latitude'] < 42) &\n                  (df_copy['longitude'] < -73) & (df_copy['longitude'] > -75)]\nprint(f\"After filtering invalid coordinates: {df_copy.shape} (removed {before_count - len(df_copy):,})\")\n\nprint(\"\\nData cleaning complete!\")\nprint(f\"Current shape: {df_copy.shape}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Check for Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate rows\n",
    "duplicates = df_copy.duplicated().sum()\n",
    "print(f\"Number of duplicate rows: {duplicates:,}\")\n",
    "\n",
    "if duplicates > 0:\n",
    "    # Remove duplicates, keeping the first occurrence\n",
    "    before_count = len(df_copy)\n",
    "    df_copy = df_copy.drop_duplicates()\n",
    "    print(f\"Duplicates removed: {before_count - len(df_copy):,}\")\n",
    "    print(f\"Final shape: {df_copy.shape}\")\n",
    "else:\n",
    "    print(\"No duplicates found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze key distributions\n",
    "print(\"INSPECTION TYPE DISTRIBUTION:\")\n",
    "print(df_copy['inspection_type'].value_counts())\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GRADE DISTRIBUTION:\")\n",
    "grade_counts = df_copy['grade'].value_counts().sort_index()\n",
    "print(grade_counts)\n",
    "print(f\"\\nGrade missing: {df_copy['grade'].isna().sum():,} ({df_copy['grade'].isna().sum()/len(df_copy)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATE RANGE:\")\n",
    "print(f\"Earliest inspection: {df_copy['inspection_date'].min()}\")\n",
    "print(f\"Latest inspection: {df_copy['inspection_date'].max()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TOP 10 CUISINES:\")\n",
    "print(df_copy['cuisine_description'].value_counts().head(10))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BOROUGH DISTRIBUTION:\")\n",
    "print(df_copy['boro'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Export Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"=\" * 60)\n",
    "print(\"FINAL CLEANED DATASET\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Shape: {df_copy.shape}\")\n",
    "print(f\"Columns: {list(df_copy.columns)}\")\n",
    "print(f\"\\nMissing values:\")\n",
    "missing = df_copy.isnull().sum()\n",
    "print(missing[missing > 0])\n",
    "\n",
    "print(\"\\nSample:\")\n",
    "print(df_copy.head(3))\n",
    "\n",
    "# Export to CSV\n",
    "output_path = '../data/cleaned_restaurant_inspections.csv'\n",
    "df_copy.to_csv(output_path, index=False)\n",
    "print(f\"\\nâœ“ Exported to: {output_path}\")\n",
    "\n",
    "df_copy.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nyc-restaurant-health-predictor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}